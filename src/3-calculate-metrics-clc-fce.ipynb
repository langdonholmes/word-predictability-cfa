{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11e47d57",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b0f45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "\n",
    "# For MTLD\n",
    "from lexicalrichness import LexicalRichness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94b1d25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original docbin...\n",
      "Loaded 2482 original documents\n",
      "Loading corrected docbin...\n",
      "Loaded 2482 corrected documents\n"
     ]
    }
   ],
   "source": [
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Load the CLC-FCE docbins\n",
    "docbin_path_original = \"../data/clc-fce-docbins/original.docbin\"\n",
    "docbin_path_corrected = \"../data/clc-fce-docbins/corrected.docbin\"\n",
    "\n",
    "print(\"Loading original docbin...\")\n",
    "docbin_original = DocBin().from_disk(docbin_path_original)\n",
    "docs_original = list(docbin_original.get_docs(nlp.vocab))\n",
    "print(f\"Loaded {len(docs_original)} original documents\")\n",
    "\n",
    "print(\"Loading corrected docbin...\")\n",
    "docbin_corrected = DocBin().from_disk(docbin_path_corrected)\n",
    "docs_corrected = list(docbin_corrected.get_docs(nlp.vocab))\n",
    "print(f\"Loaded {len(docs_corrected)} corrected documents\")\n",
    "\n",
    "assert len(docs_original) == len(docs_corrected), \"Docbins must have same number of docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b27d3799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Slim Pajama token frequencies...\n",
      "Loaded 2024311 unique tokens, total 311758665 tokens\n",
      "Loading dependency bigrams...\n",
      "Loaded 46189680 dependency bigrams, total 306325736 dependencies\n"
     ]
    }
   ],
   "source": [
    "# Load reference data\n",
    "print(\"Loading Slim Pajama token frequencies...\")\n",
    "token_freq_df = pd.read_parquet(\"../data/slim_pajama_lists/3grams.parquet\")\n",
    "\n",
    "# Need to sum counts over token_2 to get unigram frequencies\n",
    "token_freq_df = token_freq_df.groupby('token_2', as_index=False)['count'].sum()\n",
    "token_freq = dict(zip(token_freq_df['token_2'], token_freq_df['count']))\n",
    "total_tokens = sum(token_freq.values())\n",
    "print(f\"Loaded {len(token_freq)} unique tokens, total {total_tokens} tokens\")\n",
    "\n",
    "print(\"Loading dependency bigrams...\")\n",
    "dep_df = pd.read_parquet(\"../data/slim_pajama_lists/depgrams.parquet\")\n",
    "dep_counts = dep_df.set_index(['head_lemma', 'dependent_lemma', 'relation'])['count'].to_dict()\n",
    "head_marginals = dep_df.groupby('head_lemma')['count'].sum().to_dict()\n",
    "dep_marginals = dep_df.groupby('dependent_lemma')['count'].sum().to_dict()\n",
    "total_deps = dep_df['count'].sum()\n",
    "print(f\"Loaded {len(dep_counts)} dependency bigrams, total {total_deps} dependencies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ab7463",
   "metadata": {},
   "source": [
    "## 2. Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12179e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error type mapping (reasonable decisions based on description)\n",
    "# Spelling: S, SA, SX\n",
    "# Grammar: Morphological (AG, C, D, F, I), POS edits (M/R/U + POS), AS, CE, W, X, TV\n",
    "# Vocab: CL, ID, L, QL\n",
    "error_mapping = {\n",
    "    'grammar': ['AG', 'C', 'D', 'F', 'I', 'MA', 'MR', 'MU', 'MC', 'MD', 'MF', 'MI', 'MJ', 'MN', 'MP', 'MQ', 'MT', 'MV', 'MY', 'RA', 'RR', 'RU', 'RC', 'RD', 'RF', 'RI', 'RJ', 'RN', 'RP', 'RQ', 'RT', 'RV', 'RY', 'UA', 'UR', 'UU', 'UC', 'UD', 'UF', 'UI', 'UJ', 'UN', 'UP', 'UQ', 'UT', 'UV', 'UY', 'AS', 'CE', 'W', 'X', 'TV'],\n",
    "    'vocab': ['CL', 'ID', 'L', 'QL'],\n",
    "    'spelling': ['S', 'SA', 'SX']\n",
    "}\n",
    "# Flatten for lookup\n",
    "error_type_to_cat = {}\n",
    "for cat, types in error_mapping.items():\n",
    "    for t in types:\n",
    "        error_type_to_cat[t] = cat\n",
    "\n",
    "def count_errors(doc):\n",
    "    error_counts = {'error_grammar': 0, 'error_vocab': 0, 'error_spelling': 0}\n",
    "    if 'errors' in doc.spans:\n",
    "        for span in doc.spans['errors']:\n",
    "            cat = error_type_to_cat.get(span.label_, 'other')  # Default to other if not mapped\n",
    "            if not cat == 'other':\n",
    "                error_counts[f'error_{cat}'] += 1\n",
    "    return error_counts\n",
    "\n",
    "def calculate_mtld(tokens):\n",
    "    # Simple MTLD implementation (standard parameters: factor=0.72)\n",
    "    if len(tokens) < 10:\n",
    "        return np.nan\n",
    "    token_str = ' '.join(tokens)\n",
    "    lex = LexicalRichness(token_str)\n",
    "    return lex.mtld(threshold=0.72)\n",
    "\n",
    "def count_tunits(doc):\n",
    "    # T-units: each main verb (ROOT) and attached clauses\n",
    "    # For simplicity, count number of ROOT tokens (one per independent clause)\n",
    "    return sum(1 for token in doc if token.dep_ == 'ROOT')\n",
    "\n",
    "def lexical_density(doc):\n",
    "    content_pos = {'NOUN', 'VERB', 'ADJ', 'ADV'}\n",
    "    words = [t for t in doc if t.is_alpha]\n",
    "    if not words:\n",
    "        return 0\n",
    "    content_words = [t for t in words if t.pos_ in content_pos]\n",
    "    return len(content_words) / len(words)\n",
    "\n",
    "def avg_token_freq(doc, token_freq):\n",
    "    freqs = []\n",
    "    for token in doc:\n",
    "        if token.is_alpha:\n",
    "            freq = token_freq.get(token.lemma_.lower(), 1)  # Default to 1 if not found\n",
    "            freqs.append(freq)\n",
    "    return np.mean(freqs) if freqs else np.nan\n",
    "\n",
    "def mod_per_nom(doc):\n",
    "    nominals = [t for t in doc if t.pos_ == 'NOUN']\n",
    "    if not nominals:\n",
    "        return 0\n",
    "    total_mods = 0\n",
    "    for nom in nominals:\n",
    "        # Modifiers: adjectives, determiners, etc. (children with dep amod, det, etc.)\n",
    "        mods = [c for c in nom.children if c.dep_ in {'amod', 'det', 'nummod', 'compound'}]\n",
    "        total_mods += len(mods)\n",
    "    return total_mods / len(nominals)\n",
    "\n",
    "def dep_per_nom(doc):\n",
    "    nominals = [t for t in doc if t.pos_ == 'NOUN']\n",
    "    if not nominals:\n",
    "        return 0\n",
    "    total_deps = sum(len(list(nom.children)) for nom in nominals)\n",
    "    return total_deps / len(nominals)\n",
    "\n",
    "def calculate_mi(doc, dep_counts, head_marginals, dep_marginals, total_deps):\n",
    "    rel_mis = defaultdict(list)\n",
    "    for token in doc:\n",
    "        if token.dep_ in {'amod', 'advmod', 'dobj'}:\n",
    "            head = token.head.lemma_.lower()\n",
    "            dep = token.lemma_.lower()\n",
    "            rel = token.dep_\n",
    "            key = (head, dep, rel)\n",
    "            joint = dep_counts.get(key, 0)\n",
    "            if joint > 0:\n",
    "                p_joint = joint / total_deps\n",
    "                p_head = head_marginals.get(head, 0) / total_deps\n",
    "                p_dep = dep_marginals.get(dep, 0) / total_deps\n",
    "                if p_head > 0 and p_dep > 0:\n",
    "                    mi = math.log(p_joint / (p_head * p_dep))\n",
    "                    rel_mis[rel].append(mi)\n",
    "    # Average MI per relation\n",
    "    return {f'{rel}': np.mean(mis) if mis else np.nan for rel, mis in rel_mis.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2398f0",
   "metadata": {},
   "source": [
    "## 3. Process Documents and Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17000d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2482 original documents...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating metrics (original): 100%|██████████| 2482/2482 [00:02<00:00, 964.84it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2482 corrected documents...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating metrics (corrected): 100%|██████████| 2482/2482 [00:02<00:00, 1069.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Processing complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def process_docs(docs, label):\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Processing {len(docs)} {label} documents...\\n\")\n",
    "    \n",
    "    for idx, doc in tqdm(enumerate(docs), total=len(docs), desc=f\"Calculating metrics ({label})\"):\n",
    "        metrics = {'doc_id': idx}\n",
    "        \n",
    "        # Basic counts\n",
    "        words = [t for t in doc if not t.is_punct]\n",
    "        metrics['word_count'] = len(words)\n",
    "        metrics['clause_count'] = len(list(doc.sents))\n",
    "        metrics['tunit_count'] = count_tunits(doc)\n",
    "        \n",
    "        # Lexical\n",
    "        lemmas = [t.lemma_.lower() for t in words if t.is_alpha]\n",
    "        metrics['MTLD'] = calculate_mtld(lemmas)\n",
    "        metrics['lexical_density'] = lexical_density(doc)\n",
    "        metrics['token_freq'] = avg_token_freq(doc, token_freq)\n",
    "        \n",
    "        # Syntactic\n",
    "        metrics['clauses_per_tunit'] = metrics['clause_count'] / metrics['tunit_count'] if metrics['tunit_count'] > 0 else np.nan\n",
    "        metrics['mod_per_nom'] = mod_per_nom(doc)\n",
    "        metrics['dep_per_nom'] = dep_per_nom(doc)\n",
    "        \n",
    "        # MI for relations\n",
    "        mi_dict = calculate_mi(doc, dep_counts, head_marginals, dep_marginals, total_deps)\n",
    "        metrics.update({k: v for k, v in mi_dict.items()})\n",
    "        \n",
    "        # Errors\n",
    "        error_dict = count_errors(doc)\n",
    "        metrics.update(error_dict)\n",
    "        \n",
    "        results.append(metrics)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Process original docs\n",
    "df_original = process_docs(docs_original, \"original\")\n",
    "\n",
    "# Process corrected docs\n",
    "df_corrected = process_docs(docs_corrected, \"corrected\")\n",
    "\n",
    "print(\"\\n✓ Processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5e2832",
   "metadata": {},
   "source": [
    "## 4. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "749c28fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics for original metrics:\n",
      "\n",
      "            doc_id   word_count  clause_count  tunit_count         MTLD  \\\n",
      "count  2482.000000  2482.000000   2482.000000  2482.000000  2482.000000   \n",
      "mean   1240.500000   198.409750     15.213135    12.655520    57.868256   \n",
      "std     716.636007    41.823723      5.049463     4.313794    13.753058   \n",
      "min       0.000000    41.000000      2.000000     1.000000    26.237550   \n",
      "25%     620.250000   172.000000     12.000000    10.000000    47.915458   \n",
      "50%    1240.500000   192.000000     15.000000    12.000000    56.671566   \n",
      "75%    1860.750000   219.000000     18.000000    15.000000    66.063699   \n",
      "max    2481.000000   532.000000     43.000000    41.000000   132.865385   \n",
      "\n",
      "       lexical_density    token_freq  clauses_per_tunit  mod_per_nom  \\\n",
      "count      2482.000000  2.482000e+03        2482.000000  2482.000000   \n",
      "mean          0.434486  2.309365e+06           1.209515     0.753516   \n",
      "std           0.045443  4.115570e+05           0.121447     0.174122   \n",
      "min           0.317647  1.162169e+06           1.000000     0.000000   \n",
      "25%           0.402608  2.022137e+06           1.125000     0.631579   \n",
      "50%           0.429509  2.281655e+06           1.200000     0.750000   \n",
      "75%           0.463158  2.575411e+06           1.285714     0.872340   \n",
      "max           0.621429  3.711263e+06           2.000000     1.333333   \n",
      "\n",
      "       dep_per_nom         amod         dobj       advmod  error_grammar  \\\n",
      "count  2482.000000  2461.000000  2477.000000  2480.000000    2482.000000   \n",
      "mean      1.339965    -1.683450    -4.632366    -3.739280      14.611604   \n",
      "std       0.237893     1.405529     0.902457     1.344968       7.968489   \n",
      "min       0.576923    -6.975291    -8.901773   -10.280511       0.000000   \n",
      "25%       1.179615    -2.607932    -5.201446    -4.634689       9.000000   \n",
      "50%       1.333333    -1.705447    -4.625741    -3.771991      14.000000   \n",
      "75%       1.500000    -0.793582    -4.062596    -2.939071      19.000000   \n",
      "max       2.304348     4.668905     0.071299     4.389160      73.000000   \n",
      "\n",
      "       error_vocab  error_spelling  \n",
      "count  2482.000000     2482.000000  \n",
      "mean      0.210314        2.288477  \n",
      "std       0.490217        2.986295  \n",
      "min       0.000000        0.000000  \n",
      "25%       0.000000        1.000000  \n",
      "50%       0.000000        2.000000  \n",
      "75%       0.000000        3.000000  \n",
      "max       3.000000       55.000000  \n",
      "\n",
      "Summary statistics for corrected metrics:\n",
      "\n",
      "            doc_id   word_count  clause_count  tunit_count         MTLD  \\\n",
      "count  2482.000000  2482.000000   2482.000000  2482.000000  2482.000000   \n",
      "mean   1240.500000   198.599114     15.776793    13.079371    57.234720   \n",
      "std     716.636007    41.972238      5.010040     4.243750    13.497876   \n",
      "min       0.000000    41.000000      2.000000     2.000000    26.893920   \n",
      "25%     620.250000   172.250000     12.000000    10.000000    47.426844   \n",
      "50%    1240.500000   192.000000     15.000000    13.000000    55.746972   \n",
      "75%    1860.750000   219.000000     19.000000    15.000000    65.333333   \n",
      "max    2481.000000   540.000000     47.000000    41.000000   118.349191   \n",
      "\n",
      "       lexical_density    token_freq  clauses_per_tunit  mod_per_nom  \\\n",
      "count      2482.000000  2.482000e+03        2482.000000  2482.000000   \n",
      "mean          0.432555  2.338102e+06           1.212323     0.772203   \n",
      "std           0.044743  3.977661e+05           0.116696     0.166610   \n",
      "min           0.306358  1.115005e+06           1.000000     0.000000   \n",
      "25%           0.400000  2.062163e+06           1.133333     0.656250   \n",
      "50%           0.427443  2.305995e+06           1.200000     0.768336   \n",
      "75%           0.461467  2.596182e+06           1.285714     0.888889   \n",
      "max           0.625000  3.874952e+06           2.000000     1.360000   \n",
      "\n",
      "       dep_per_nom         dobj         amod       advmod  error_grammar  \\\n",
      "count  2482.000000  2481.000000  2464.000000  2482.000000         2482.0   \n",
      "mean      1.361014    -4.566566    -1.549443    -3.598219            0.0   \n",
      "std       0.231511     0.862475     1.385725     1.291427            0.0   \n",
      "min       0.625000    -9.699076    -6.975291    -7.917077            0.0   \n",
      "25%       1.208576    -5.108182    -2.464682    -4.469590            0.0   \n",
      "50%       1.357143    -4.574306    -1.559185    -3.603900            0.0   \n",
      "75%       1.500000    -4.016565    -0.683837    -2.815758            0.0   \n",
      "max       2.304348    -0.940176     4.431531     4.130346            0.0   \n",
      "\n",
      "       error_vocab  error_spelling  \n",
      "count       2482.0          2482.0  \n",
      "mean           0.0             0.0  \n",
      "std            0.0             0.0  \n",
      "min            0.0             0.0  \n",
      "25%            0.0             0.0  \n",
      "50%            0.0             0.0  \n",
      "75%            0.0             0.0  \n",
      "max            0.0             0.0  \n"
     ]
    }
   ],
   "source": [
    "# Display summary statistics\n",
    "print(\"Summary statistics for original metrics:\\n\")\n",
    "print(df_original.describe())\n",
    "\n",
    "print(\"\\nSummary statistics for corrected metrics:\\n\")\n",
    "print(df_corrected.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f488a8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Original results saved to: ../data/clc_fce_metrics_original.csv\n",
      "✓ Corrected results saved to: ../data/clc_fce_metrics_corrected.csv\n",
      "  Total docs: 2482\n"
     ]
    }
   ],
   "source": [
    "# Save to files\n",
    "output_path_original = \"../data/clc_fce_metrics_original.csv\"\n",
    "output_path_corrected = \"../data/clc_fce_metrics_corrected.csv\"\n",
    "\n",
    "df_original.to_csv(output_path_original, index=False)\n",
    "df_corrected.to_csv(output_path_corrected, index=False)\n",
    "\n",
    "print(f\"✓ Original results saved to: {output_path_original}\")\n",
    "print(f\"✓ Corrected results saved to: {output_path_corrected}\")\n",
    "print(f\"  Total docs: {len(df_original)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
